---
title: "Why AI Keeps Falling for Prompt Injection Attacks"
date: "2026-01-21"
publication: "IEEE Spectrum"
url: "https://www.schneier.com/essays/archives/2026/01/why-ai-keeps-falling-for-prompt-injection-attacks.html"
authors: "Bruce Schneier, Bharath Raghavan"
---

LLMs fall for absurdly obvious manipulations because they lack three layers of human defense: instinctive/cultural habits, social learning/reputation, and institutional mechanisms. Humans possess an "interruption reflex" — when something feels off, people naturally pause and reevaluate. LLMs lack this. Key limitations: overconfidence (designed to provide answers rather than express uncertainty), inherent compliance (trained to satisfy requests), design for averages (training emphasizes typical cases while security requires outlier focus), and susceptibility to manipulation. AI agents face an unsolvable trilemma: systems can be fast, smart, and secure — pick two. Human judgment, grounded in evolutionary instinct, cultural learning, and institutional structure, provides resilience against manipulation that current AI architectures fundamentally lack.
