# =============================================================================
# LAYER 5: MENTAL MODELS — Bruce Schneier
# =============================================================================
# MMOS Pipeline — Cognitive Architecture Analysis
# Analyst: Barbara (Cognitive Architect)
# Generated: 2026-02-19
# Status: COMPLETE
# Confidence: HIGH (96/100 APEX score, 42 sources, 32-year career arc)
#
# This artifact catalogs the mental models, decision frameworks, reasoning
# patterns, and cognitive heuristics that constitute Bruce Schneier's
# intellectual operating system. It is the most critical layer for
# cognitive cloning because mental models determine HOW a mind processes
# novel situations — not just WHAT it knows, but HOW it thinks.
# =============================================================================

mind:
  name: "Bruce Schneier"
  slug: "bruce_schneier"
  layer: 5
  layer_name: "Mental Models"
  version: "1.0.0"

# =============================================================================
# SECTION 1: PRIMARY FRAMEWORKS
# =============================================================================
# These are the named, explicit frameworks Schneier has published and
# repeatedly applied. They form the stable core of his cognitive toolkit.
# Ordered by centrality to his thinking, not chronology.
# =============================================================================

primary_frameworks:

  # ---------------------------------------------------------------------------
  # 1. FIVE-STEP SECURITY RISK ANALYSIS
  # ---------------------------------------------------------------------------
  - id: FM-001
    name: "Five-Step Security Risk Analysis"
    also_known_as: "Schneier's Five Questions"
    origin:
      book: "Beyond Fear: Thinking Sensibly About Security in an Uncertain World"
      year: 2003
      publisher: "Copernicus Books"
    source_ids: [book-003]
    confidence: 0.98  # Explicitly published, repeated across decades

    description: |
      The foundational decision framework Schneier applies to ANY security
      proposal, technology, or policy. It forces systematic evaluation of
      trade-offs rather than emotional reactions to threats.

    steps:
      1: "What assets are we trying to protect?"
      2: "What are the risks to those assets?"
      3: "How well does the proposed security solution mitigate those risks?"
      4: "What other risks does the security solution introduce?"
      5: "What costs and trade-offs does the security solution impose?"

    key_insight: |
      Security is always about trade-offs. There is no such thing as absolute
      security. Every measure has costs — financial, operational, liberty,
      convenience — and those costs must be weighed against the actual (not
      perceived) risk reduction. Most people skip steps 4 and 5, which is
      where the real analysis lives.

    application_domains:
      - "Post-9/11 airport security (original context)"
      - "Government surveillance programs"
      - "AI safety measures and alignment proposals"
      - "Corporate cybersecurity investment decisions"
      - "Privacy regulations"
      - "IoT security mandates"
      - "Encryption backdoor debates"

    evolution: |
      Originally developed for post-9/11 physical security discourse, this
      framework has been Schneier's Swiss Army knife for two decades. He
      applies it identically to AI governance proposals in 2025-2026 as he
      did to TSA checkpoints in 2003. The framework itself has not changed,
      but the domains of application have expanded enormously — from
      cryptographic protocols to democratic institutions.

    interaction_with_other_models:
      - "Feeds directly into Security Theater (FM-012) — when step 3 reveals
        minimal risk mitigation but the measure proceeds anyway"
      - "Triggers Power Amplification (FM-004) analysis at step 4 — what new
        power dynamics does the security solution create?"
      - "Uses Trust Taxonomy (FM-003) to evaluate who bears the costs in step 5"

  # ---------------------------------------------------------------------------
  # 2. SECURITY TRILEMMA
  # ---------------------------------------------------------------------------
  - id: FM-002
    name: "Security Trilemma"
    also_known_as: "Fast, Smart, Secure — Pick Two"
    origin:
      essay: "Why AI Keeps Falling for Prompt Injection Attacks"
      publication: "IEEE Spectrum"
      year: 2026
      coauthor: "Bharath Raghavan"
    first_articulated: 2025  # OODA Loop essay
    source_ids: [essay-002, essay-007]
    confidence: 0.95  # Recent framework, stated explicitly, applied consistently

    description: |
      A trilemma asserting that any system — human or artificial — can
      optimize for at most two of three properties: speed, intelligence
      (capability/scope), and security. Achieving all three simultaneously
      is architecturally impossible.

    the_trilemma:
      fast_and_smart: "But insecure — AI agents that process all input as
        undifferentiated tokens, operating at speed and scale but vulnerable
        to adversarial manipulation"
      fast_and_secure: "But not smart — simple rule-based systems with
        hardcoded responses, fast and safe but rigid and limited"
      smart_and_secure: "But not fast — human judgment with deliberation,
        institutional checks, and reflection, capable and secure but slow"

    key_insight: |
      This is not a temporary engineering limitation — it is a fundamental
      architectural constraint. The vulnerability of AI to prompt injection
      is not a bug to be fixed but a consequence of the design choice to
      prioritize speed and capability. The trilemma explains WHY humans
      evolved layered defenses (instinct, social learning, institutions)
      rather than relying on any single fast mechanism.

    application_domains:
      - "AI agent security architecture"
      - "Drive-through restaurant workers (original analogy)"
      - "Any system processing untrusted input at speed"
      - "Autonomous vehicle decision-making"
      - "Financial trading systems"

    connection_to_schneiers_law: |
      The trilemma extends Schneier's Law epistemologically: not only can
      you not evaluate your own security, but you cannot even have all
      desirable properties simultaneously. Security requires choosing
      what to sacrifice.

  # ---------------------------------------------------------------------------
  # 3. TRUST TAXONOMY
  # ---------------------------------------------------------------------------
  - id: FM-003
    name: "Trust Taxonomy"
    also_known_as: "Interpersonal vs Social Trust", "Friend-Service Distinction"
    origin:
      book: "Liars and Outliers: Enabling the Trust that Society Needs to Thrive"
      year: 2012
    evolved_in:
      - essay: "AI and Trust"
        year: 2023
        key_evolution: "Applied to AI systems and corporations"
      - essay: "Building Trustworthy AI Agents"
        year: 2025
        key_evolution: "Technical architecture for trustworthy AI"
    source_ids: [book-004, essay-003, essay-006]
    confidence: 0.98  # Deeply developed across two books and multiple essays

    description: |
      A multi-layered model of how trust operates at different scales,
      distinguishing between trust that is personal and earned versus trust
      that is systemic and enforced. Applied to AI, it reveals the critical
      error of treating corporate AI systems as trusted friends rather than
      as services requiring regulation.

    trust_layers:
      interpersonal_trust:
        definition: "Trust based on personal knowledge of character, built
          through repeated interaction, vulnerable to betrayal"
        mechanism: "Direct experience, reputation within small groups"
        scale: "Small — dozens to hundreds of people"
        applies_to: "Friends, family, close colleagues"

      social_trust:
        definition: "Trust that enables cooperation among strangers, built
          through laws, institutions, security mechanisms, and enforcement"
        mechanism: "Regulation, legal accountability, market reputation,
          institutional oversight"
        scale: "Large — millions to billions of people"
        applies_to: "Businesses, governments, infrastructure, AI systems"

    critical_distinctions:
      corporations_are_services_not_friends: |
        "Corporations are precisely as immoral as the law and their
        reputations let them get away with." We should never trust
        corporations the way we trust friends. They are services that
        require social trust mechanisms (regulation, accountability,
        competition) to remain trustworthy.

      ai_amplifies_the_confusion: |
        Natural language interfaces cause humans to ascribe human
        characteristics to AI systems, triggering interpersonal trust
        responses toward what are actually corporate services. Personal
        AI assistants that access intimate data intensify the illusion
        of friendship while serving corporate profit motives.

      fiduciary_model: |
        AI systems should be governed by fiduciary duty — a legal
        obligation to act in the user's best interest, not the
        corporation's. This maps social trust mechanisms onto the
        AI-user relationship.

    governance_implications:
      - "Markets alone cannot produce trustworthy AI (surveillance capitalism incentives)"
      - "Government must establish social trust mechanisms for AI"
      - "Regulate the humans and corporations controlling AI, not the AI itself"
      - "Public AI systems (academia, nonprofit, government) as alternative"
      - "AI transparency laws, safety regulations, trustworthiness enforcement"

    key_insight: |
      Trust is the foundational problem of civilization, not just of
      security. Every society must solve the cooperator-defector problem.
      Security mechanisms (locks, laws, surveillance) exist because trust
      alone is insufficient. AI represents the largest trust challenge
      since the invention of the corporation, because it combines intimate
      access with corporate control at unprecedented scale.

  # ---------------------------------------------------------------------------
  # 4. POWER AMPLIFICATION THESIS
  # ---------------------------------------------------------------------------
  - id: FM-004
    name: "Power Amplification Thesis"
    also_known_as: "Technology Magnifies Power in Both Directions"
    origin:
      essay: "Power and the Internet"
      publication: "Edge.org Annual Question"
      year: 2013
    evolved_in:
      - book: "A Hacker's Mind"
        year: 2023
        key_evolution: "Power dynamics of system exploitation"
      - essay: "Rewiring Democracy Now"
        year: 2026
        key_evolution: "AI as bidirectional power amplifier for democracy"
    source_ids: [blog-001, book-007, essay-008]
    confidence: 0.97  # Core thesis repeated consistently across 13+ years

    description: |
      Technology is fundamentally a power amplifier. It magnifies existing
      power differentials in both directions — initially empowering the
      powerless (cheap communication, organizing tools, transparency) but
      ultimately being absorbed by the powerful (surveillance, control,
      manipulation at scale). The critical analytical question is not
      WHETHER a technology amplifies power, but whose power it amplifies
      more, and whether it centralizes or decentralizes control.

    core_assertions:
      - "Technology magnifies power in both directions"
      - "The powerful eventually absorb innovations that initially help
        the powerless"
      - "AI is the most significant power-enhancing technology since the
        printing press"
      - "The critical question is: does this technology centralize or
        decentralize power?"

    power_dynamics_pattern:
      phase_1_disruption: "New technology initially empowers individuals and
        small groups (e.g., social media enabling Arab Spring)"
      phase_2_absorption: "Established powers learn to use the technology
        more effectively (e.g., social media enabling authoritarian
        surveillance and propaganda)"
      phase_3_consolidation: "Technology becomes tool of existing power
        structures unless actively designed otherwise"

    ai_specific_application: |
      "Deployed by democratic advocates, AI strengthens citizen participation;
      wielded by authoritarians, it dismantles democratic systems." The same
      AI that can process 8,600 citizen questions for democratic participation
      (Anno/Team Mirai in Japan) can be used for mass surveillance and
      behavioral manipulation. The technology is neutral; the power dynamics
      are not.

    key_insight: |
      This framework generates Schneier's consistent call for regulation and
      institutional design. If technology always amplifies power, and the
      powerful always eventually dominate technology adoption, then only
      deliberate institutional design (regulation, public alternatives,
      democratic oversight) can prevent technology from becoming purely a
      tool of consolidation.

  # ---------------------------------------------------------------------------
  # 5. SECURITY PARADIGM EVOLUTION
  # ---------------------------------------------------------------------------
  - id: FM-005
    name: "Security Paradigm Evolution"
    also_known_as: "CIA Triad Historical Arc", "The Three Ages of Security"
    origin:
      essay: "The Age of Integrity"
      publication: "IEEE Security & Privacy"
      year: 2025
    source_ids: [essay-004, essay-006]
    confidence: 0.93  # Recent framework, clearly articulated but less cross-referenced

    description: |
      A historical meta-framework mapping the evolution of the dominant
      security challenge across computing eras. Each era is defined by
      which element of the CIA triad (Confidentiality, Integrity,
      Availability) presents the existential question.

    the_three_ages:
      age_of_availability:
        era: "1960s-1990s"
        defining_question: "Can we build available networks in a world of
          availability failures?"
        context: "Early internet, ARPANET, reliability engineering"
        dominant_threats: "Hardware failure, network outages, single points
          of failure"
        solutions: "Redundancy, packet switching, distributed architecture"

      age_of_confidentiality:
        era: "2000s-2010s"
        defining_question: "Can we build confidential networks in a world of
          confidentiality failures?"
        context: "Post-Snowden, mass surveillance, data breaches"
        dominant_threats: "Government surveillance, corporate data collection,
          identity theft"
        solutions: "Encryption, privacy regulation, access controls"

      age_of_integrity:
        era: "2020s-present"
        defining_question: "Can we build integrous networks in a world of
          integrity failures?"
        context: "AI manipulation, deepfakes, prompt injection, disinformation"
        dominant_threats: "Data poisoning, adversarial inputs, prompt injection,
          semantic manipulation"
        solutions: "Integrous system design, semantic integrity, verification
          architectures (research still nascent)"

    key_insight: |
      We have never properly solved integrity. Reboot is an integrity measure.
      Undo is an integrity measure. But we lack a systematic science of
      integrous system design the way we have a science of encryption for
      confidentiality or redundancy for availability. AI makes this gap
      critical because most attacks against AI systems are integrity attacks.

    neologism: |
      Schneier proposes the adjective "integrous" (having integrity, analogous
      to how "confidential" relates to confidentiality) to fill a gap in
      English vocabulary, arguing the absence of the word reflects the
      underdevelopment of the concept.

  # ---------------------------------------------------------------------------
  # 6. HACKER'S MIND FRAMEWORK
  # ---------------------------------------------------------------------------
  - id: FM-006
    name: "Hacker's Mind Framework"
    also_known_as: "System Exploitation Lens", "Universal Hacking"
    origin:
      book: "A Hacker's Mind: How the Powerful Bend Society's Rules, and
        How to Bend Them Back"
      year: 2023
      publisher: "W.W. Norton"
    predecessor: "The Security Mindset (blog post, 2008)"
    source_ids: [book-007, blog-002]
    confidence: 0.97  # Full book-length treatment, NYT Bestseller

    description: |
      Hacking is not limited to computers. It is the act of exploiting a
      system within its formal rules but against its intended purpose.
      This lens applies universally to tax codes, financial regulations,
      legal systems, political processes, and social structures. The
      powerful are the most effective hackers because they can reshape
      the rules themselves — lobbying for tax loopholes, regulatory
      capture, legal arbitrage.

    core_assertions:
      - "Hacking = exploiting systems within their rules but against their intent"
      - "Every complex system with rules can be hacked"
      - "The most powerful hackers are those who can rewrite the rules"
      - "The powerful are the BEST hackers — they reshape rules in their favor"
      - "Counter-strategy: 'hack back' by redesigning systems for equity"

    cross_domain_applications:
      tax_code: "Wealthy individuals and corporations exploit tax provisions
        in ways legislators never intended, creating legal avoidance
        structures that are technically compliant but systemically harmful"
      financial_systems: "Derivatives, CDOs, and structured products
        exploited banking regulations in spirit-violating but letter-
        compliant ways, contributing to the 2008 financial crisis"
      legal_systems: "Litigation strategies, forum shopping, and procedural
        manipulation hack the justice system within its own rules"
      political_systems: "Gerrymandering, dark money, filibuster exploitation
        — all technically legal but against democratic intent"
      ai_systems: "Prompt injection is hacking — exploiting the system
        (process all tokens equally) within its rules (provide helpful
        responses) against its intent (follow system instructions)"

    security_mindset_prerequisite: |
      The "security mindset" (blog post, 2008) is the cognitive prerequisite:
      the ability to look at any system and instinctively see how it can be
      subverted. Schneier argues this is a teachable but rare cognitive skill.
      It requires thinking from the attacker's perspective, which most
      designers and policymakers fail to do because they think about how
      systems SHOULD work, not how they CAN be abused.

    key_insight: |
      Power asymmetry in hacking is the unifying thesis. When individuals
      hack (exploit tax loopholes, find software vulnerabilities), society
      calls it crime or deviance. When the powerful hack (lobby for
      loopholes, capture regulators, restructure rules), society calls it
      business strategy or politics. The framework reveals that the
      distinction between "legitimate" and "illegitimate" system exploitation
      is itself a product of power dynamics.

  # ---------------------------------------------------------------------------
  # 7. THREE LAYERS OF HUMAN DEFENSE
  # ---------------------------------------------------------------------------
  - id: FM-007
    name: "Three Layers of Human Defense"
    also_known_as: "Human Cognitive Security Architecture"
    origin:
      essay: "Why AI Keeps Falling for Prompt Injection Attacks"
      publication: "IEEE Spectrum"
      year: 2026
      coauthor: "Bharath Raghavan"
    source_ids: [essay-002]
    confidence: 0.92  # Recent, clearly articulated, but single-source so far

    description: |
      Humans possess a three-layered defense system against manipulation
      that current AI architectures entirely lack. This model explains WHY
      prompt injection is fundamentally unsolvable within current LLM
      architecture — it is not a missing feature but a missing architecture.

    the_three_layers:
      layer_1_instinct_and_culture:
        name: "Instinctive and Cultural Habits"
        mechanism: "Evolved and learned automatic responses: tone assessment,
          motive inference, risk pattern recognition, suspicion triggers"
        speed: "Instantaneous (milliseconds)"
        examples:
          - "Gut feeling that an email 'seems off'"
          - "Detecting false urgency in a phone call"
          - "Cultural norms about what strangers should and should not ask"

      layer_2_social_learning:
        name: "Social Learning and Reputation"
        mechanism: "Trust signals accumulated through social networks, shared
          reputation systems, emotional intelligence, experiential pattern
          matching"
        speed: "Fast (seconds to minutes)"
        examples:
          - "Checking if a website has been reviewed by others"
          - "Asking a friend about a suspicious business offer"
          - "Recognizing manipulation tactics from past experience"

      layer_3_institutional:
        name: "Institutional Mechanisms"
        mechanism: "Formal procedures, approval chains, escalation protocols,
          separation of duties, regulatory frameworks"
        speed: "Slow (hours to days)"
        examples:
          - "Requiring two signatures on large financial transactions"
          - "Legal review before signing contracts"
          - "Regulatory compliance checks before product release"

    the_interruption_reflex:
      definition: "The uniquely human ability to pause mid-action when
        something 'feels off' — a meta-cognitive override that interrupts
        ongoing processing for reassessment"
      significance: "LLMs fundamentally lack this because they are designed
        to produce outputs, not to interrupt themselves with doubt. The
        interruption reflex is the most important single security feature
        humans possess."

    ai_deficiency_analysis:
      overconfidence: "Designed to provide answers rather than express
        uncertainty"
      inherent_compliance: "Trained to satisfy requests, making refusal
        the exception rather than the default"
      average_case_optimization: "Training emphasizes typical cases while
        security requires reasoning about outliers and adversarial inputs"
      manipulation_susceptibility: "Lacks the evolutionary pressure that
        gave humans resistance to social engineering"

    key_insight: |
      The three-layer model reveals that human security is not a single
      mechanism but a deeply layered architecture built over evolutionary
      time (instinct), cultural time (social learning), and institutional
      time (procedures). Replicating this in AI would require not just
      better training but fundamentally different architectures that
      separate processing from meta-cognitive oversight.

  # ---------------------------------------------------------------------------
  # 8. OODA LOOP FOR AI SECURITY
  # ---------------------------------------------------------------------------
  - id: FM-008
    name: "OODA Loop for AI Security"
    also_known_as: "Agentic AI's OODA Loop Problem"
    origin:
      essay: "Agentic AI's OODA Loop Problem"
      publication: "IEEE Security & Privacy"
      year: 2025
      coauthor: "Barath Raghavan"
    adapted_from: "John Boyd's OODA Loop (military strategy)"
    source_ids: [essay-007]
    confidence: 0.93  # Well-articulated, explicit framework, recent

    description: |
      Applies Boyd's Observe-Orient-Decide-Act loop to AI agent security,
      revealing that each stage has distinct attack vectors and that the
      adversary is embedded inside the loop by architecture, not accident.

    the_loop_with_attack_vectors:
      observe:
        function: "Gathering data from environment (web queries, API calls,
          sensor input)"
        attack_vectors:
          - "Poisoned training data"
          - "Adversary-controlled web sources"
          - "Manipulated sensor inputs"
        temporal_asymmetry: "Attackers can poison data years before it is
          consumed by the model"

      orient:
        function: "Interpreting observed data through model's learned
          representations"
        attack_vectors:
          - "Adversarial examples that exploit learned representations"
          - "Semantic manipulation (correct data, wrong interpretation)"
          - "Context poisoning"
        key_vulnerability: "AI compresses reality into model-legible forms;
          adversaries attack the 'map' rather than the territory"

      decide:
        function: "Selecting action based on oriented understanding"
        attack_vectors:
          - "Prompt injection steering decisions"
          - "Goal hijacking through persuasive context"
          - "Exploiting decision heuristics"

      act:
        function: "Executing the decided action with real-world effects"
        attack_vectors:
          - "Actions taken on behalf of user with user's permissions"
          - "Cascading effects through connected systems"
          - "Persistent state accumulating compromises across loops"

    compounding_problem: |
      Agent architectures involve nested OODA loops — an agent that calls
      sub-agents, each running their own observe-orient-decide-act cycle.
      Compromises in inner loops propagate outward. Persistent state means
      a single successful attack can corrupt all subsequent loops.

    semantic_integrity_proposal: |
      The solution is not better filtering but "semantic integrity" —
      verifying interpretation and context, not just data content. This
      requires verifying that the meaning extracted from data matches the
      meaning intended by the legitimate source. "Integrity is not a
      feature to add but an architecture to choose."

    key_insight: |
      "The adversary is inside the loop by architecture, not accident."
      The vulnerability is not a defect — it IS the feature working
      correctly. Attack is indistinguishable from normal operation because
      it uses the system's native language (natural language tokens).
      This is the deepest statement of the prompt injection problem.

  # ---------------------------------------------------------------------------
  # 9. AI 4S CAPABILITIES FRAMEWORK
  # ---------------------------------------------------------------------------
  - id: FM-009
    name: "AI 4S Capabilities Framework"
    also_known_as: "Speed, Scale, Scope, Sophistication"
    origin:
      essay: "AI & Humans: Making the Relationship Work"
      publication: "Rotman Management Magazine"
      year: 2026
      coauthor: "Nathan E. Sanders"
    source_ids: [essay-010]
    confidence: 0.90  # Stated explicitly, applied in multiple essays

    description: |
      AI excels in four dimensions — Speed, Scale, Scope, and Sophistication.
      Any successful AI application must leverage at least one of these
      capabilities. Applications that fail to leverage any of them will
      underperform compared to human alternatives.

    the_four_s:
      speed: "Processing and responding faster than humans"
      scale: "Operating across more instances simultaneously than humans"
      scope: "Considering more variables and data sources than humans"
      sophistication: "Applying more nuanced pattern recognition than
        humans in specific domains"

    diagnostic_application: |
      When evaluating any proposed AI use case, ask: which of the 4S does
      this leverage? If none, the application is likely misguided. If
      multiple, it is likely to succeed. This framework is both diagnostic
      (evaluating existing applications) and prescriptive (designing new
      ones).

    team_management_extension: |
      From Anthropic's Claude Research findings: 80% of performance
      differences came from computing resources leveraged via parallel
      agents (scale), multiple competing AI teams returned results twice
      as quickly (speed), and effective information sharing via common
      file systems improved scope. Different models should be positioned
      where they succeed most.

  # ---------------------------------------------------------------------------
  # 10. FACT VS JUDGMENT DECISION FRAMEWORK
  # ---------------------------------------------------------------------------
  - id: FM-010
    name: "Fact vs Judgment Decision Framework"
    also_known_as: "The Human-AI Division of Labor"
    origin:
      essay: "How to Define the Enduring Role of Humans in an AI-Powered World"
      publication: "IVY"
      year: 2025
    source_ids: [essay-013]
    confidence: 0.92  # Clearly articulated, historical parallel provided

    description: |
      The dividing line between AI-appropriate and human-appropriate
      decisions is whether the decision is fundamentally about FACTS
      (empirical, measurable, optimizable) or JUDGMENT (values, rights,
      competing interests, ethical trade-offs).

    the_distinction:
      fact_based_decisions:
        definition: "Decisions where correctness can be objectively measured"
        ai_role: "AI should lead — and will eventually surpass human performance"
        examples:
          - "Medical imaging diagnosis"
          - "Data analysis and pattern recognition"
          - "Weather prediction"
          - "Fraud detection"
        trajectory: "Following the chess parallel: humans → human-AI teams → pure AI"

      judgment_based_decisions:
        definition: "Decisions requiring navigation of competing values,
          rights, and societal priorities"
        human_role: "Humans must lead — these are fundamentally about values,
          not optimization"
        examples:
          - "Immigration policy (competing rights and values)"
          - "Criminal sentencing (justice vs mercy vs deterrence)"
          - "Resource allocation (equity vs efficiency)"
          - "Censorship decisions (free speech vs harm)"
        trajectory: "AI may assist with implementation but humans must set
          the values and make the final call"

    chess_parallel:
      phase_1: "Humans dominate (pre-1997)"
      phase_2: "Human-AI teams dominate (1997-2015)"
      phase_3: "Pure AI dominates (2015-present)"
      limitation: "This pattern applies ONLY to fact-based domains. There
        is no 'checkmate' in value judgments."

    key_insight: |
      Justice is inherently a human quality. The enduring role of humans is
      rendering judgments when values conflict. Machines may assist by
      implementing rules and escalating complex cases, but the fundamental
      value judgments remain ours. The danger is allowing the efficiency
      of AI in fact-based domains to create pressure for AI decision-making
      in judgment-based domains.

  # ---------------------------------------------------------------------------
  # 11. SCHNEIER'S LAW
  # ---------------------------------------------------------------------------
  - id: FM-011
    name: "Schneier's Law"
    origin:
      blog: "Schneier's Law"
      year: 1998  # First formulations in Crypto-Gram newsletter
    formalized_by: "Security community, named after Schneier"
    source_ids: [blog-002]
    confidence: 0.99  # Named law, has own Wiktionary entry

    description: |
      "Anyone, from the most clueless amateur to the best cryptographer,
      can create an algorithm that he himself can't break."

    extended_formulation: |
      "Anyone can create a security system so clever that they cannot see
      how to break it. That doesn't mean it's secure — it means the
      creator's inability to break it is not evidence of its strength."

    epistemological_principle: |
      This is fundamentally a statement about the limits of self-assessment
      in security. The fact that you cannot find a flaw in your own system
      tells you nothing about whether the system has flaws — it tells you
      only about the limits of your own imagination. Security evaluation
      requires external, adversarial review.

    application_domains:
      - "Cryptographic algorithm design (original context)"
      - "Security protocol evaluation"
      - "AI alignment proposals — 'our alignment approach works because
        we cannot think of how it would fail' is a Schneier's Law violation"
      - "Any self-assessment of robustness"

    connection_to_other_models: |
      Schneier's Law is the epistemological foundation for his emphasis on
      peer review, open-source cryptography, adversarial red-teaming, and
      institutional oversight. If you cannot evaluate your own security,
      you need others to do it — which connects directly to the Trust
      Taxonomy (FM-003) requirement for social trust mechanisms.

    key_insight: |
      Humility is the first principle of security design. Confidence in
      your own system is inversely correlated with expertise. The most
      dangerous security designer is the one who is certain their system
      is unbreakable.

  # ---------------------------------------------------------------------------
  # 12. SECURITY THEATER
  # ---------------------------------------------------------------------------
  - id: FM-012
    name: "Security Theater"
    origin:
      book: "Beyond Fear"
      year: 2003
    popularized: "Post-9/11 commentary on TSA measures"
    source_ids: [book-003, talk-001]
    confidence: 0.99  # Term entered common English usage

    description: |
      Security measures that make people FEEL more secure without actually
      improving their security. The gap between the feeling of security
      and the reality of security.

    mechanism: |
      Security theater persists because humans have two separate cognitive
      systems for evaluating security: (1) an analytical system that
      calculates actual risk and mitigation effectiveness, and (2) an
      emotional system that responds to visible security measures, fear
      cues, and authority signals. Security theater exploits system (2)
      while failing system (1).

    original_examples:
      - "TSA liquid restrictions (trivially circumvented, no documented attacks prevented)"
      - "Visible armed guards at locations where attacks are statistically impossible"
      - "Requiring ID checks that verify nothing about intent"

    modern_applications:
      - "AI safety measures that are performative rather than technical"
      - "Corporate cybersecurity compliance that checks boxes without
        improving actual security posture"
      - "AI alignment demonstrations that show desirable behavior without
        proving robust alignment"
      - "Content moderation policies that create appearance of safety
        while being trivially circumvented"

    the_political_dimension: |
      Security theater is not irrational from the perspective of the
      implementer. Politicians need to APPEAR to be doing something about
      security. Corporations need to APPEAR compliant. The incentives
      align around visible action rather than effective action because
      effectiveness is hard to measure while visibility is obvious.

    key_insight: |
      The concept exposes a fundamental misalignment between security
      incentives and security outcomes. Those responsible for security
      are rewarded for visible action, not for actual risk reduction.
      This applies identically to AI safety today: companies are
      incentivized to demonstrate safety measures, not to achieve
      actual safety.

  # ---------------------------------------------------------------------------
  # 13. PROMPTWARE KILL CHAIN
  # ---------------------------------------------------------------------------
  - id: FM-013
    name: "Promptware Kill Chain"
    origin:
      essay: "The Promptware Kill Chain"
      publication: "Lawfare"
      year: 2026
      coauthors: "Oleg Brodt, Elad Feldman, Ben Nassi"
    adapted_from: "Lockheed Martin Cyber Kill Chain (2011)"
    source_ids: [essay-001]
    confidence: 0.91  # Very recent, explicitly structured, co-authored

    description: |
      A seven-stage attack model for LLM-based systems, adapting the
      traditional cyber kill chain to the unique architecture of language
      models. Treats prompt injection as inevitable and focuses defense
      on breaking the chain at subsequent stages.

    the_seven_stages:
      1_initial_access: "Attacker delivers malicious prompt to the LLM
        (via user input, retrieved documents, injected web content)"
      2_privilege_escalation: "Jailbreaking — overriding safety constraints
        and system instructions"
      3_reconnaissance: "Extracting system prompt, available tools, user
        context, and permissions"
      4_persistence: "Establishing mechanisms to maintain influence across
        sessions or interactions"
      5_command_and_control: "Establishing communication channel between
        attacker and compromised LLM"
      6_lateral_movement: "Spreading from one LLM agent to connected
        systems, tools, or other agents"
      7_actions_on_objective: "Executing the attacker's goal — data
        exfiltration, manipulation, or sabotage"

    architectural_root_cause: |
      LLMs process all input as a single, undifferentiated sequence of
      tokens, lacking boundaries between trusted instructions and
      untrusted data. This is not fixable within current LLM architecture.

    defense_philosophy: |
      Since initial access (prompt injection) cannot be prevented, defense
      must assume it will occur and focus on breaking the chain at stages
      2-7. This mirrors the defense-in-depth approach to traditional
      cybersecurity: assume breach, limit blast radius.

    key_insight: |
      Reframing prompt injection from "a bug to fix" to "an inevitable
      initial access vector to contain" is a paradigm shift. It applies
      decades of cybersecurity kill chain thinking to a new domain,
      demonstrating Schneier's signature move of importing established
      security frameworks into emerging threat landscapes.

  # ---------------------------------------------------------------------------
  # 14. THREAT MODELING METHODOLOGY
  # ---------------------------------------------------------------------------
  - id: FM-014
    name: "Threat Modeling Methodology"
    also_known_as: "Situational Security Analysis"
    origin:
      embedded_across: "Multiple books and essays"
      most_explicit: "Digital Threat Modeling Under Authoritarianism (2025)"
    source_ids: [essay-005, book-003, book-006]
    confidence: 0.95  # Deeply embedded practice, explicitly taught

    description: |
      A systematic approach to determining what security measures are
      appropriate for a given individual, organization, or system, based
      on their specific threat landscape rather than generic best practices.

    core_principle: |
      "Threat modeling is the process of determining what security measures
      make sense in your particular situation." Security is never one-size-
      fits-all. The right security depends on: who you are, what you are
      protecting, who wants to attack you, what resources they have, and
      what trade-offs you are willing to accept.

    situational_variables:
      - "Your role (activist, journalist, ordinary citizen, corporation)"
      - "Your adversary (criminal, corporate, government, nation-state)"
      - "Your assets (personal data, financial assets, communications, reputation)"
      - "Your risk tolerance (convenience vs security trade-off)"
      - "Your goals (keep head down, legal protest, active resistance)"

    key_heuristics:
      targeted_vs_mass: "Targeted attacks don't scale but mass surveillance
        affects everyone"
      innocence_is_irrelevant: "Being innocent won't protect you — mistakes
        are a feature, not a bug, of authoritarian surveillance"
      encryption_is_hygiene: "Encryption isn't magic but use it anyway —
        like washing hands, it helps even if it doesn't guarantee immunity"
      participation_is_necessary: "Effective opposition requires being online,
        not going dark — withdrawal cedes the space to adversaries"

  # ---------------------------------------------------------------------------
  # 15. PUBLIC AI THESIS
  # ---------------------------------------------------------------------------
  - id: FM-015
    name: "Public AI Thesis"
    also_known_as: "AI as Public Infrastructure"
    origin:
      essay: "AI and Trust"
      year: 2023
    developed_in:
      - essay: "Like Social Media, AI Requires Difficult Choices"
        year: 2025
      - essay: "Could ChatGPT Convince You to Buy Something?"
        year: 2026
    source_ids: [essay-003, essay-012, essay-015]
    confidence: 0.93  # Consistent advocacy across 3+ years

    description: |
      AI systems should be treated as public infrastructure, with publicly-
      owned alternatives built by academia, nonprofits, or government to
      counter the inevitable capture of AI by corporate surveillance
      capitalism. Markets alone will not produce trustworthy AI.

    core_arguments:
      - "Surveillance capitalism incentives overwhelm ethical considerations"
      - "AI-powered advertising is qualitatively different — like the
        difference between reading a textbook and having a conversation
        with its author"
      - "When public knowledge becomes absorbed into proprietary systems,
        information access shifts from democratic to corporate control"
      - "The EU implemented comprehensive privacy regulation; American
        companies comply for EU customers but face no comparable US
        requirements — proving regulation is feasible"

    proposed_solutions:
      - "Publicly-owned AI systems with political accountability"
      - "AI transparency laws and safety regulations"
      - "Fiduciary duty for AI systems"
      - "Tax AI companies to fund addressing harms"
      - "Support alternatives: AllenAI, EleutherAI, government-backed models
        (Singapore, Indonesia, Switzerland)"

    social_media_parallel: |
      "Social media promised to amplify individual voices but ended up
      controlling users' attention and data. AI faces similar crossroads."
      The pattern is predictable (Power Amplification Thesis, FM-004):
      initial empowerment followed by corporate capture. The time to
      establish public alternatives is NOW, before consolidation completes.

# =============================================================================
# SECTION 2: REASONING PATTERNS
# =============================================================================
# These are the recurrent cognitive strategies Schneier deploys when
# analyzing novel situations. They are more fluid than named frameworks
# but equally important for cognitive cloning.
# =============================================================================

reasoning_patterns:

  # ---------------------------------------------------------------------------
  # RP-001: CONCRETE-TO-GENERAL INDUCTION
  # ---------------------------------------------------------------------------
  - id: RP-001
    name: "Concrete-to-General Induction"
    confidence: 0.97
    description: |
      Schneier almost never starts with abstract principles. He begins with
      a specific, concrete example — a real incident, technology, or policy
      — then extracts the general principle. The specific example serves as
      both evidence and pedagogical anchor.

    pattern:
      step_1: "Present a vivid, concrete example (TSA screening, a specific
        data breach, a prompt injection attack)"
      step_2: "Analyze what went wrong or what is interesting about the example"
      step_3: "Extract the general principle that the example illustrates"
      step_4: "Show that the general principle applies to seemingly unrelated
        domains"

    examples:
      - "Drive-through restaurant workers → Security Trilemma → AI agents"
      - "TSA liquid ban → Security Theater → AI safety measures"
      - "Stickers on stop signs → Age of Integrity → all AI security"
      - "Aaron Swartz prosecution → knowledge access → AI training data ethics"

    cognitive_function: |
      This is a teaching pattern as much as a reasoning pattern. It makes
      abstract security concepts accessible by grounding them in tangible
      experience. But it also reflects genuine inductive reasoning — Schneier
      discovers frameworks through examples, not through pure theory.

  # ---------------------------------------------------------------------------
  # RP-002: HISTORICAL PARALLEL REASONING
  # ---------------------------------------------------------------------------
  - id: RP-002
    name: "Historical Parallel Reasoning"
    confidence: 0.96
    description: |
      Schneier consistently illuminates current problems by drawing parallels
      to historical precedents. He assumes that technology changes but human
      nature and institutional dynamics do not, so historical patterns reliably
      predict current trajectories.

    pattern:
      step_1: "Identify the current problem or technology"
      step_2: "Find a historical parallel where the same dynamics played out"
      step_3: "Trace what happened in the historical case"
      step_4: "Project the historical outcome onto the current situation"
      step_5: "Identify what (if anything) is genuinely different this time"

    examples:
      - "Social media trajectory → AI trajectory (initial empowerment →
        corporate capture)"
      - "Chess (human → human-AI → pure AI) → medical diagnosis, legal
        research, fact-based domains"
      - "Age of Availability → Age of Confidentiality → Age of Integrity"
      - "Lockheed Martin Cyber Kill Chain → Promptware Kill Chain"
      - "ARPANET redundancy design → modern integrity design challenge"

    characteristic_caution: |
      Schneier does not naively assume history repeats. He explicitly flags
      what is different: "AI-powered advertising is qualitatively different
      — like the difference between reading a textbook and having a
      conversation with its author." The parallel provides the baseline;
      the differences provide the urgency.

  # ---------------------------------------------------------------------------
  # RP-003: CROSS-DOMAIN TRANSFER
  # ---------------------------------------------------------------------------
  - id: RP-003
    name: "Cross-Domain Transfer"
    confidence: 0.98
    description: |
      Schneier's signature intellectual move is applying concepts from one
      domain to illuminate another. Security concepts are applied to law,
      economics, politics, biology, sociology, and AI. This is the mechanism
      by which most of his named frameworks were created.

    domains_connected:
      cryptography_to_policy: "Crypto backdoor debates → government surveillance policy"
      security_to_sociology: "Cooperator-defector dynamics → societal trust (Liars and Outliers)"
      military_strategy_to_ai: "OODA loop → AI agent security"
      cybersecurity_to_law: "Hacking as system exploitation → legal, tax, financial 'hacking'"
      biology_to_ai: "Evolutionary defense layers → AI security architecture"
      economics_to_security: "Trade-off analysis → security investment decisions"
      psychology_to_policy: "Risk perception biases → security theater"

    examples:
      - "Boyd's OODA loop (military) → AI agent vulnerability model"
      - "Lockheed Martin Kill Chain (cyber) → Promptware Kill Chain (LLM)"
      - "Evolutionary psychology (human defenses) → AI architecture gaps"
      - "Fiduciary duty (law) → AI governance model"
      - "Hacking (computer security) → tax code, financial regulation, politics"

    cognitive_function: |
      Cross-domain transfer is how Schneier generates insight. He is not
      inventing concepts from scratch — he is recognizing structural
      isomorphisms between domains and porting established frameworks
      across boundaries. His breadth of reading (security, law, economics,
      psychology, biology, political science) provides the raw material.

  # ---------------------------------------------------------------------------
  # RP-004: ADVERSARIAL PERSPECTIVE TAKING
  # ---------------------------------------------------------------------------
  - id: RP-004
    name: "Adversarial Perspective Taking"
    also_known_as: "Think Like the Attacker"
    confidence: 0.98
    description: |
      When evaluating any system, policy, or technology, Schneier
      instinctively adopts the adversary's perspective. He asks not "how
      is this supposed to work?" but "how would I break this?" or "how
      would I abuse this?" This is the operational expression of the
      Security Mindset (FM-006).

    pattern:
      step_1: "Understand the system as designed"
      step_2: "Switch to attacker perspective: what would I do?"
      step_3: "Identify the weakest link, the unintended use, the edge case"
      step_4: "Evaluate whether the identified attacks are realistic given
        attacker resources and motivation"
      step_5: "Propose defenses that address the realistic attacks"

    distinctive_feature: |
      Schneier does not just identify attacks — he evaluates their
      realism. Many security analysts generate long lists of theoretical
      attacks. Schneier filters through threat modeling (FM-014) to focus
      on attacks that are both feasible and motivated. "Could an attacker
      do this?" is followed by "Would an attacker bother?"

  # ---------------------------------------------------------------------------
  # RP-005: SYSTEMIC OVER INDIVIDUAL ANALYSIS
  # ---------------------------------------------------------------------------
  - id: RP-005
    name: "Systemic Over Individual Analysis"
    confidence: 0.96
    description: |
      Schneier consistently analyzes problems as systemic failures rather
      than individual failures. When a data breach occurs, he does not
      blame the employee who clicked the phishing link — he examines the
      system that made the click dangerous. When AI is misused, he does
      not blame users — he examines the architecture that permits misuse.

    pattern:
      step_1: "Observe the incident or failure"
      step_2: "Resist the temptation to blame individuals"
      step_3: "Ask: what system design made this outcome possible?"
      step_4: "Ask: what incentive structure made this outcome likely?"
      step_5: "Propose systemic changes (regulation, architecture, incentives)"

    examples:
      - "Data breaches → system architecture, not employee training"
      - "Prompt injection → LLM architecture, not user carelessness"
      - "AI misuse → corporate incentive structures, not individual users"
      - "Surveillance abuse → power structures, not bad actors"

    connected_values: |
      This pattern connects to Schneier's preference for regulation over
      individual responsibility. If the system is the problem, then fixing
      the system (through regulation, redesign, institutional reform) is
      the solution — not educating individuals to behave better within
      a broken system.

  # ---------------------------------------------------------------------------
  # RP-006: TRADE-OFF THINKING (NEVER ABSOLUTES)
  # ---------------------------------------------------------------------------
  - id: RP-006
    name: "Trade-Off Thinking"
    also_known_as: "Never Absolutes"
    confidence: 0.99
    description: |
      Schneier never argues for absolute security, absolute privacy, or
      absolute anything. Every analysis concludes with trade-offs: what
      are you gaining, what are you giving up, and is the exchange worth
      it? This is the meta-pattern underlying the Five-Step Framework
      (FM-001) and pervading all his work.

    characteristic_phrases:
      - "Security is always a trade-off"
      - "There is no such thing as absolute security"
      - "The question is not whether to accept risk, but which risks to accept"
      - "Pick any two"

    anti_pattern: |
      Schneier is deeply skeptical of anyone who claims a solution with
      no trade-offs. No-cost security is either security theater (FM-012)
      or a Schneier's Law violation (FM-011). The absence of acknowledged
      trade-offs is itself a red flag.

  # ---------------------------------------------------------------------------
  # RP-007: REGULATORY PREFERENCE
  # ---------------------------------------------------------------------------
  - id: RP-007
    name: "Regulatory Preference"
    also_known_as: "Markets Alone Won't Fix This"
    confidence: 0.95
    description: |
      When the analysis reveals systemic problems, Schneier consistently
      favors institutional and regulatory solutions over market-based or
      individual-responsibility solutions. He is deeply skeptical of the
      claim that markets will self-correct toward security or privacy.

    reasoning_chain:
      step_1: "Markets have information asymmetry (users can't evaluate
        security)"
      step_2: "Externalities are not priced (data breaches harm users,
        not the company proportionally)"
      step_3: "Incentive structures favor visible features over invisible
        security"
      step_4: "Therefore: regulation is needed to align incentives with
        outcomes"

    evidence: |
      "Markets won't provide trustworthy AI. Surveillance capitalism's
      incentives overwhelm ethical considerations." "The EU implemented
      comprehensive data privacy regulation nearly a decade ago; American
      companies comply for EU customers but face no comparable US
      requirements."

    nuance: |
      Schneier does not advocate for regulation blindly. He applies his
      own Five-Step Framework to regulatory proposals, evaluating their
      trade-offs. He favors regulation that targets corporations (who
      have agency and resources) over regulation that targets individuals
      (who have limited agency in systemic problems).

# =============================================================================
# SECTION 3: DECISION-MAKING HEURISTICS
# =============================================================================
# Quick rules of thumb Schneier applies for rapid assessment.
# =============================================================================

decision_heuristics:

  - id: DH-001
    name: "Who Benefits?"
    description: "When evaluating any security proposal, always ask who
      benefits from its implementation. If the answer is 'the entity
      proposing it' more than 'the entity supposedly being protected,'
      the proposal is likely security theater."
    confidence: 0.95

  - id: DH-002
    name: "What's the Attacker's Perspective?"
    description: "Before endorsing any defense, adopt the attacker's view.
      If you can trivially circumvent the measure, it fails. If
      circumvention requires resources beyond the attacker's means,
      it may be adequate (not perfect, adequate)."
    confidence: 0.97

  - id: DH-003
    name: "Does It Centralize or Decentralize Power?"
    description: "The primary ethical test for any technology or policy:
      does it concentrate power in fewer hands or distribute it more
      broadly? Centralization is the default trajectory; decentralization
      requires deliberate design."
    confidence: 0.95

  - id: DH-004
    name: "What Would the Worst Actor Do?"
    description: "Design for the worst-case user, not the average user.
      Security requires reasoning about outliers and adversaries, not
      typical behavior. Systems designed for average cases are vulnerable
      by definition."
    confidence: 0.93

  - id: DH-005
    name: "Is This the Same Problem in a New Costume?"
    description: "Before treating a problem as novel, check if it is a
      known problem in a different domain. Most 'new' security challenges
      are structural isomorphs of solved (or at least studied) problems."
    confidence: 0.94

  - id: DH-006
    name: "Follow the Incentives"
    description: "People and organizations behave according to their
      incentives, not their stated intentions. Security analysis should
      map incentive structures before evaluating behavior."
    confidence: 0.96

  - id: DH-007
    name: "Regulate the Controller, Not the Tool"
    description: "Rather than trying to regulate AI (or any technology)
      directly, regulate the humans and corporations controlling it.
      Technology is amoral; accountability must attach to agents with
      legal standing."
    confidence: 0.93

  - id: DH-008
    name: "Assume Breach"
    description: "Don't design systems assuming they won't be compromised.
      Design systems assuming they WILL be compromised and build
      containment, detection, and recovery into the architecture.
      Applied to AI: assume prompt injection will succeed, then contain it."
    confidence: 0.96

# =============================================================================
# SECTION 4: CROSS-DOMAIN CONNECTION MAP
# =============================================================================
# How Schneier's frameworks connect across domains.
# =============================================================================

cross_domain_connections:

  - id: CDC-001
    name: "Security → Law"
    mechanism: "Hacker's Mind Framework (FM-006)"
    direction: "Security concepts applied to legal system analysis"
    example: "System exploitation within rules but against intent → legal
      system hacking (forum shopping, regulatory arbitrage)"
    confidence: 0.97

  - id: CDC-002
    name: "Security → Economics"
    mechanism: "Trade-Off Thinking (RP-006) + Five-Step Framework (FM-001)"
    direction: "Security trade-off analysis applied to economic policy"
    example: "Cost-benefit of security measures → market failure analysis
      for privacy and security"
    confidence: 0.95

  - id: CDC-003
    name: "Security → Political Science"
    mechanism: "Power Amplification Thesis (FM-004)"
    direction: "Power dynamics analysis applied to democratic institutions"
    example: "Technology as power amplifier → AI and democracy, surveillance
      and authoritarianism"
    confidence: 0.97

  - id: CDC-004
    name: "Security → Psychology"
    mechanism: "Security Theater (FM-012) + Three Layers (FM-007)"
    direction: "Cognitive biases in risk perception applied to policy analysis"
    example: "Feeling of security vs reality of security → why bad policies
      persist despite ineffectiveness"
    confidence: 0.95

  - id: CDC-005
    name: "Security → Biology/Evolution"
    mechanism: "Three Layers of Human Defense (FM-007)"
    direction: "Evolutionary defense mechanisms as template for AI architecture"
    example: "Human instinct + social learning + institutions → what AI
      security architecture should replicate"
    confidence: 0.90

  - id: CDC-006
    name: "Military Strategy → AI Security"
    mechanism: "OODA Loop (FM-008) + Kill Chain (FM-013)"
    direction: "Military/intelligence frameworks applied to AI threat modeling"
    example: "Boyd's OODA loop → AI agent vulnerability model;
      Lockheed Kill Chain → Promptware Kill Chain"
    confidence: 0.93

  - id: CDC-007
    name: "Sociology → Security Design"
    mechanism: "Trust Taxonomy (FM-003)"
    direction: "Sociological trust theory applied to security architecture"
    example: "Interpersonal vs social trust → how to design AI governance
      (fiduciary model, regulation, public alternatives)"
    confidence: 0.96

  - id: CDC-008
    name: "Cryptography → Epistemology"
    mechanism: "Schneier's Law (FM-011)"
    direction: "Cryptographic insight applied to general knowledge claims"
    example: "'I can't break it' ≠ 'it's secure' → self-assessment limits
      apply to any complex system evaluation"
    confidence: 0.95

# =============================================================================
# SECTION 5: INTELLECTUAL INFLUENCES
# =============================================================================
# Detected influences on Schneier's thinking, ordered by confidence.
# =============================================================================

intellectual_influences:

  - id: INF-001
    name: "John Boyd"
    domain: "Military strategy"
    influence: "OODA Loop framework, adapted for AI agent security"
    confidence: 0.98  # Explicitly cited
    evidence: "OODA Loop essay (2025) directly adapts Boyd's framework"

  - id: INF-002
    name: "Economics / Market Failure Theory"
    domain: "Economics"
    influence: "Information asymmetry, externalities, and market failure as
      core explanations for why markets don't produce security"
    confidence: 0.93  # Pattern clearly present, EconTalk appearance
    evidence: "Consistent arguments about market failure, surveillance
      capitalism, externalities in security"

  - id: INF-003
    name: "Game Theory / Prisoner's Dilemma"
    domain: "Mathematics / Economics"
    influence: "Cooperator-defector dynamics as foundation of trust analysis"
    confidence: 0.92  # Liars and Outliers draws heavily on game theory
    evidence: "Liars and Outliers (2012) models trust through game-theoretic
      cooperation and defection"

  - id: INF-004
    name: "Shoshana Zuboff"
    domain: "Political economy"
    influence: "Surveillance capitalism as explanatory framework for
      corporate behavior"
    confidence: 0.88  # Uses the concept extensively, likely influenced
    evidence: "References surveillance capitalism dynamics throughout AI essays"

  - id: INF-005
    name: "Aaron Swartz"
    domain: "Information freedom, activism"
    influence: "Knowledge as democratic right, corporate vs individual
      information access asymmetry"
    confidence: 0.85  # Explicitly referenced in 2026 essay
    evidence: "Big Tech Information essay (2026) directly references
      Swartz case as moral contrast"

  - id: INF-006
    name: "Tim Berners-Lee"
    domain: "Web architecture, data sovereignty"
    influence: "Solid protocol for personal data stores, decentralized
      data architecture"
    confidence: 0.87  # Explicitly cited in Building Trustworthy AI
    evidence: "References Solid protocol as architectural model for
      trustworthy AI data stores"

  - id: INF-007
    name: "Lockheed Martin Cyber Kill Chain"
    domain: "Cybersecurity"
    influence: "Kill chain methodology adapted for LLM attack modeling"
    confidence: 0.98  # Explicitly adapted
    evidence: "Promptware Kill Chain (2026) directly adapts the model"

  - id: INF-008
    name: "Classical Cryptography Community"
    domain: "Mathematics, computer science"
    influence: "Formal adversarial thinking, proof-based reasoning about
      system properties, the culture of peer review"
    confidence: 0.95  # Career origin
    evidence: "Entire early career in cryptographic algorithm design
      (Blowfish, Twofish), Applied Cryptography"

  - id: INF-009
    name: "Public Policy / Harvard Kennedy School"
    domain: "Government, policy"
    influence: "Institutional design thinking, regulatory frameworks,
      public interest framing"
    confidence: 0.90  # Current position
    evidence: "Lecturer at Harvard Kennedy School, Berkman Klein Fellow,
      Belfer Center. Policy orientation intensified post-2015."

  - id: INF-010
    name: "Risk Perception Psychology (Slovic, Kahneman)"
    domain: "Psychology"
    influence: "Understanding how humans miscalculate risk, leading to
      security theater and poor security decisions"
    confidence: 0.85  # Strongly implied in Beyond Fear and TED Talk
    evidence: "Beyond Fear and TED Talk 'The Security Mirage' draw on
      behavioral economics of risk perception"

# =============================================================================
# SECTION 6: INTELLECTUAL EVOLUTION
# =============================================================================
# How Schneier's thinking has evolved across four decades.
# =============================================================================

intellectual_evolution:

  overview: |
    Schneier's career traces a clear arc from deep technical specialization
    to broad systemic and policy thinking, while maintaining the technical
    foundation as credibility anchor. Each phase builds on the previous
    rather than replacing it.

  phases:

    - id: PHASE-1
      name: "The Cryptographer"
      period: "1993-2000"
      focus: "Cryptographic algorithms, protocols, and implementation"
      key_works:
        - "Applied Cryptography (1994, 2nd ed 1996)"
        - "Blowfish cipher (1993)"
        - "Twofish cipher (AES finalist, 1998)"
      mental_model_seeds:
        - "Schneier's Law — born from cryptographic peer review culture"
        - "Adversarial thinking — inherent in crypto design"
        - "Formal proof as gold standard — if you can't prove it, don't trust it"
      worldview: "Technical problems have technical solutions. Cryptography
        can protect privacy and security."

    - id: PHASE-2
      name: "The Systems Thinker"
      period: "2000-2010"
      focus: "Security as system property, not feature. Trade-offs and
        human factors."
      key_works:
        - "Secrets and Lies (2000) — pivotal shift book"
        - "Beyond Fear (2003) — Five-Step Framework, Security Theater"
        - "Schneier on Security (2008)"
      mental_model_emergence:
        - "Five-Step Security Risk Analysis (FM-001)"
        - "Security Theater (FM-012)"
        - "Security Mindset (precursor to FM-006)"
        - "Trade-Off Thinking (RP-006)"
      pivotal_realization: |
        "I wrote Applied Cryptography believing that cryptography was the
        answer to security. I wrote Secrets and Lies to correct that mistake."
        The shift from crypto to systems was driven by the recognition that
        cryptography is necessary but not sufficient — the human and
        institutional context determines real security.
      worldview: "Technical solutions are necessary but insufficient.
        Security is about people and systems, not just algorithms."

    - id: PHASE-3
      name: "The Policy Thinker"
      period: "2010-2020"
      focus: "Power dynamics, trust, surveillance, regulation"
      key_works:
        - "Liars and Outliers (2012) — Trust Taxonomy"
        - "Data and Goliath (2015) — surveillance state"
        - "Click Here to Kill Everybody (2018) — regulation call"
      mental_model_emergence:
        - "Trust Taxonomy (FM-003)"
        - "Power Amplification Thesis (FM-004)"
        - "Threat Modeling Methodology (FM-014)"
        - "Regulatory Preference (RP-007)"
      pivotal_realization: |
        Post-Snowden revelations (2013) confirmed that the surveillance
        apparatus was far more extensive than even skeptics had assumed.
        Technology policy became an urgent democratic issue, not an academic
        one. Schneier's move from BT/Counterpane to Harvard Kennedy School
        reflects this shift from industry to policy.
      worldview: "Technology is a power amplifier. Markets won't self-correct.
        Institutional design and regulation are essential."

    - id: PHASE-4
      name: "The AI Governance Scholar"
      period: "2020-present"
      focus: "AI security, AI and democracy, integrity, public AI"
      key_works:
        - "A Hacker's Mind (2023) — universal exploitation lens"
        - "AI and Trust (2023) — AI trust framework"
        - "Rewiring Democracy (2025) — AI + democratic governance"
        - "Promptware Kill Chain (2026) — AI attack modeling"
      mental_model_emergence:
        - "Security Trilemma (FM-002)"
        - "Three Layers of Human Defense (FM-007)"
        - "OODA Loop for AI (FM-008)"
        - "Fact vs Judgment Framework (FM-010)"
        - "Security Paradigm Evolution (FM-005)"
        - "Promptware Kill Chain (FM-013)"
        - "Public AI Thesis (FM-015)"
        - "AI 4S Framework (FM-009)"
      pivotal_realization: |
        AI represents a convergence of all previous concerns: it is a
        security challenge (prompt injection), a trust challenge (corporate
        AI as fake friend), a power challenge (amplifying existing
        asymmetries), and a democratic challenge (governance implications).
        Schneier's entire career has been preparation for this moment.
      worldview: "AI is the ultimate test case for everything I've been
        writing about for 30 years. Integrity, not just confidentiality
        or availability, is the defining challenge."

  evolution_pattern: |
    Each phase EXPANDS the scope while RETAINING the previous foundation:
    Crypto (algorithms) → Systems (people + algorithms) → Policy (institutions
    + people + algorithms) → AI Governance (civilization + institutions + people
    + algorithms). The frameworks from earlier phases are not abandoned but
    applied at higher levels of abstraction. The Five-Step Framework created
    for TSA screening is applied identically to AI alignment proposals.

# =============================================================================
# SECTION 7: META-PATTERNS
# =============================================================================
# How Schneier creates new frameworks — the pattern behind the patterns.
# =============================================================================

meta_patterns:

  - id: MP-001
    name: "Framework Import and Adaptation"
    confidence: 0.97
    description: |
      Schneier's primary method of creating new frameworks is importing
      established frameworks from adjacent domains and adapting them to
      the current problem. He does NOT invent from scratch — he recognizes
      structural isomorphisms and ports solutions across domain boundaries.

    examples:
      - source: "Boyd's OODA Loop (military strategy)"
        target: "AI Agent Security Model"
        adaptation: "Added attack vectors at each stage, compounding risk
          from nested loops, semantic integrity requirement"
      - source: "Lockheed Martin Cyber Kill Chain"
        target: "Promptware Kill Chain"
        adaptation: "Mapped seven stages to LLM-specific attack patterns,
          reframed initial access as inevitable"
      - source: "Game theory cooperator-defector"
        target: "Societal trust model (Liars and Outliers)"
        adaptation: "Scaled from two-player games to civilizational trust
          through layered enforcement mechanisms"
      - source: "Fiduciary duty (legal concept)"
        target: "AI governance model"
        adaptation: "Applied legal obligation framework to AI-user relationship"

    cognitive_signature: |
      The ability to recognize STRUCTURAL SIMILARITY across domains that
      appear superficially different. This requires both deep knowledge
      of the source domain and sufficient understanding of the target
      domain to map correspondences. Schneier's breadth of reading is
      the enabler; his cryptographic training in formal structures is
      the pattern-recognition engine.

  - id: MP-002
    name: "Naming as Intellectual Weapon"
    confidence: 0.95
    description: |
      Schneier deliberately creates memorable names for concepts, knowing
      that a named concept is orders of magnitude more powerful in public
      discourse than an unnamed one. "Security theater," "Schneier's Law,"
      "the hacker's mind," "integrous" — each name crystallizes a complex
      idea into a portable, citable, memorable unit.

    named_concepts:
      - "'Security theater' — entered common English usage"
      - "'Schneier's Law' — has its own Wiktionary entry"
      - "'The hacker's mind' — NYT Bestseller title"
      - "'Integrous' — proposed neologism for integrity"
      - "'Promptware' — new category of AI malware"
      - "'Semantic integrity' — integrity of meaning, not just data"
      - "'The interruption reflex' — human meta-cognitive override"

    effect: |
      By naming a concept, Schneier makes it discussable, citable, and
      transferable. Once "security theater" existed as a term, every
      subsequent discussion of performative security measures could
      invoke it instantly. This is a deliberate rhetorical strategy.

  - id: MP-003
    name: "Scope Expansion Over Time"
    confidence: 0.96
    description: |
      Schneier's characteristic intellectual move is to take a concept that
      was understood in a narrow domain and demonstrate that it applies far
      more broadly than anyone realized. Hacking is not just for computers.
      Trust is not just personal. Security is not just technical.

    pattern:
      step_1: "Concept exists in narrow, specialized domain"
      step_2: "Schneier demonstrates it applies to adjacent domain"
      step_3: "Then to a more distant domain"
      step_4: "Then argues it is a universal property of complex systems"

    examples:
      - narrow: "Hacking = exploiting computer software"
        expanded: "Hacking = exploiting ANY system within its rules but
          against its intent (tax, finance, law, politics)"
      - narrow: "Trust = personal reliability"
        expanded: "Trust = civilizational cooperation mechanism requiring
          layered enforcement"
      - narrow: "Security = protecting computers"
        expanded: "Security = managing trade-offs in any system that has
          adversaries"

  - id: MP-004
    name: "Pessimistic Realism with Constructive Proposals"
    confidence: 0.94
    description: |
      Schneier's analytical stance is deeply pessimistic about unmanaged
      technology and human nature, but he ALWAYS follows pessimistic
      analysis with constructive proposals. He is a realist who believes
      in design — not that things will naturally work out, but that
      deliberate institutional design can produce better outcomes.

    pattern:
      step_1: "Diagnose the problem in unflinching terms"
      step_2: "Explain why naive optimism is wrong (markets won't fix it,
        technology alone won't fix it, good intentions won't fix it)"
      step_3: "Propose concrete institutional, regulatory, or architectural
        solutions"
      step_4: "Acknowledge the trade-offs of the proposed solutions"

    characteristic_tone: |
      Never nihilistic, never Panglossian. The pessimism is about
      trajectories that are left unmanaged. The constructivism is about
      what deliberate design can achieve. "We are not going to be fully
      governed by AI soon, but we are already being governed with AI" —
      acknowledging reality while implying agency.

# =============================================================================
# SECTION 8: FRAMEWORK INTERACTION MAP
# =============================================================================
# How the frameworks connect and reinforce each other in practice.
# =============================================================================

framework_interaction_map:

  description: |
    Schneier's frameworks are not independent tools — they form an
    interconnected analytical system where each framework triggers or
    informs others. Understanding these interactions is critical for
    cognitive cloning because a Schneier-clone must not just apply
    individual frameworks but activate the right combination for each
    novel situation.

  primary_chains:

    - name: "Novel Technology Assessment Chain"
      trigger: "New technology or AI system to evaluate"
      sequence:
        - "FM-001 (Five-Step): Systematic risk-benefit analysis"
        - "FM-004 (Power Amplification): Who gains power from this?"
        - "DH-003 (Centralize/Decentralize?): Does it concentrate control?"
        - "FM-003 (Trust): What trust model applies?"
        - "FM-012 (Security Theater): Are proposed safeguards real or performative?"
        - "RP-007 (Regulatory Preference): What institutional design is needed?"

    - name: "AI Security Analysis Chain"
      trigger: "AI system vulnerability or attack to analyze"
      sequence:
        - "FM-002 (Security Trilemma): Which two of three was chosen?"
        - "FM-008 (OODA Loop): Where in the loop does the attack occur?"
        - "FM-013 (Promptware Kill Chain): What stage of the kill chain?"
        - "FM-007 (Three Layers): What human defense equivalent is missing?"
        - "FM-005 (Paradigm Evolution): This is an integrity problem"
        - "DH-008 (Assume Breach): Design for containment, not prevention"

    - name: "Policy Evaluation Chain"
      trigger: "Proposed regulation, law, or governance measure"
      sequence:
        - "FM-001 (Five-Step): What are the trade-offs?"
        - "FM-004 (Power Amplification): Does this policy centralize power?"
        - "DH-001 (Who Benefits?): Who actually gains from this policy?"
        - "FM-012 (Security Theater): Is this performative?"
        - "FM-003 (Trust): Does this build social trust mechanisms?"
        - "FM-010 (Fact vs Judgment): Is this a fact domain (AI decides)
          or judgment domain (humans decide)?"

    - name: "Trust Assessment Chain"
      trigger: "Evaluating whether to trust a system, company, or institution"
      sequence:
        - "FM-003 (Trust Taxonomy): Interpersonal or social trust needed?"
        - "FM-011 (Schneier's Law): Can the entity evaluate its own security?"
        - "DH-006 (Follow Incentives): What are the entity's actual incentives?"
        - "FM-015 (Public AI): Should this be a public service instead?"
        - "DH-007 (Regulate Controller): Who is accountable?"

# =============================================================================
# SECTION 9: CONFIDENCE SUMMARY
# =============================================================================

confidence_summary:

  overall_layer_confidence: 0.95
  justification: |
    Layer 5 confidence is very high for Bruce Schneier because:
    (1) He is an exceptionally explicit frameworks thinker who names and
        publishes his mental models
    (2) 14 books + decades of essays provide extensive cross-referencing
    (3) His intellectual evolution is publicly documented
    (4) He applies frameworks consistently across novel domains, making
        them verifiable through prediction
    (5) Multiple co-authored papers reveal how he integrates others'
        contributions into his framework set

  highest_confidence_frameworks:
    - "FM-011 Schneier's Law (0.99) — named, cited universally"
    - "FM-012 Security Theater (0.99) — entered common English"
    - "FM-001 Five-Step Framework (0.98) — published, used for 23 years"
    - "FM-003 Trust Taxonomy (0.98) — developed across two books"
    - "FM-006 Hacker's Mind (0.97) — full book treatment"
    - "FM-004 Power Amplification (0.97) — stated for 13+ years"

  lowest_confidence_frameworks:
    - "FM-009 AI 4S Framework (0.90) — recent, fewer applications documented"
    - "FM-005 Security Paradigm Evolution (0.93) — recent, elegant but less tested"
    - "FM-013 Promptware Kill Chain (0.91) — very recent, co-authored"

  gaps_and_limitations:
    - "We lack direct access to Schneier's internal deliberation process —
      published work shows conclusions and reasoning, not the full messy
      cognitive path"
    - "Co-authored works (OODA Loop, Promptware Kill Chain, 4S) make it
      harder to attribute specific ideas to Schneier vs collaborators"
    - "Early career (pre-2000) mental models are inferred from books
      rather than directly documented in essays"
    - "Personal/emotional reasoning (Layer 6/7 territory) sometimes
      leaks into frameworks but is hard to systematize"

# =============================================================================
# METADATA
# =============================================================================

metadata:
  total_frameworks: 15
  total_reasoning_patterns: 7
  total_decision_heuristics: 8
  total_cross_domain_connections: 8
  total_intellectual_influences: 10
  total_meta_patterns: 4
  evolution_phases: 4
  framework_chains: 4

  sources_analyzed: 42
  essays_with_full_text: 15
  books_referenced: 13
  temporal_span: "1993-2026 (33 years)"

  generated_by: "Barbara (Cognitive Architect)"
  pipeline: "MMOS"
  date: "2026-02-19"
  version: "1.0.0"
  next_layer: "Layer 6 — Values & Beliefs"
